{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"jA6sxZvU0pU_"},"source":["# **DLIP Tutorial - PyTorch**\n","# Transfer Learning using Pre-trained Models (Classification) - Part 2\n","\n","Y.-K. Kim\n","(updated 2022. 5. 10) \n","\n","===================\n","\n","- Part1: inference using pre-trained model\n","\n","- **Part2: Transfer Learning using Pre-trained Models (Classification)**\n","\n"," : The purpose of this tutorial is to learn how to **transfer learning** using a pre-trained model.\n","\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"gDPCnTHL16ac"},"source":["In this document we will perform two types of **transfer learning**: \n","- **finetuning**: update all parameters of the pretrained model for our new task\n","- **feature extraction**: only update the final layer weights from which we derive predictions"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"feUhurhuXLJn"},"source":["# Environment Setup\n","\n","we will mount 'Google Drive' to load Python modules and image data. \n","- [download modules](https://drive.google.com/file/d/1hjrWkcvBTiI-5yGtWPvsYVdaE7YLNWDo/view?usp=sharing)\n","- [download dataset(ant/bee)](https://drive.google.com/file/d/123qUnqUpSzpnj7BnJjftFClmK6PLRzfA/view?usp=sharing)\n","\n","upload *resources* to the following path in Google Drive. **'MyDrive/HGU_DLIP'**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Ml-yjzwy2V0M"},"source":["![image](https://user-images.githubusercontent.com/23421059/167506132-b4b921e0-661f-4a11-b4b4-4790d82dac54.png)\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"elapsed":2959,"status":"ok","timestamp":1683618995030,"user":{"displayName":"민웅한","userId":"00573162135069363404"},"user_tz":-540},"id":"Yy3bLKtZ7O1O","outputId":"f826aecb-afd9-4a59-bffa-2b3d18aaaece"},"outputs":[{"name":"stdout","output_type":"stream","text":["C:\\Users\\hanmu\\Desktop\\DLIP FILES\\DeepLearning_TUTORIAL\\4.Pytorch_TransferLearning\n"]}],"source":["'''\n","    Google colab의 경우, google drive에 저장된 파일에 쉽게 접근할 수 있도록 설계된 환경\n","    Local jupyter notebook에서 해당 데이터 파일에 접근할 수 있도록 코드를 수정\n","'''\n","\n","import os\n","# from google.colab import drive\n","# drive.mount('/content/drive')\n","# root_path = '/content/drive/MyDrive/DLIP'  # change dir to your project folder\n","# os.chdir(root_path)  #change dir\n","# os.getcwd()\n","\n","root_path = r\"C:/Users/hanmu/Desktop/DLIP FILES/DeepLearning_TUTORIAL/4.Pytorch_TransferLearning\"  # change dir to your project folder\n","os.chdir(root_path)         #change dir\n","print(os.getcwd())          # print current dir"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":906,"status":"ok","timestamp":1683618999055,"user":{"displayName":"민웅한","userId":"00573162135069363404"},"user_tz":-540},"id":"VkrQAZxK6_lL","outputId":"30b4a3aa-4be9-428d-a848-f2925927a829"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cuda device\n","Device name: NVIDIA GeForce RTX 3070 Laptop GPU\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","import torchvision.transforms as transforms\n","from torchvision import models\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# from set_parameter_requires_grad import set_parameter_requires_grad\n","from initialize_model import initialize_model\n","from train import train\n","from _test import test\n","\n","# Get cpu or gpu device for training.\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","print(f\"Using {device} device\")\n","if torch.cuda.is_available(): print(f'Device name: {torch.cuda.get_device_name(0)}') \n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"KuWFu1dRT9ME"},"source":["# MODEL"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Xu7t3PHjw7S_"},"source":["Basically, the classification models provided by torchvision are trained on ImageNet and consist of 1000 output layers.\n","\n","However, in the model for fine-tuning with other datasets, the number of output layers should be different depending on the class.\n","\n","Here, we use the initialize_model() function provided in the [pytorch tutorial](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html) to change the output stage of the model.\n","\n","initialize_model() is a function that helps to initialize the fine-tuning of some models.\n","\n","If the model is not in the function, the output layer information can be known by printing the model with the print() function."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":471,"status":"ok","timestamp":1683619006249,"user":{"displayName":"민웅한","userId":"00573162135069363404"},"user_tz":-540},"id":"CfAzSQm_7e-3","outputId":"39f68d34-0889-4274-902e-5dcf4db2726a"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\hanmu\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","C:\\Users\\hanmu\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\hanmu/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [01:06<00:00, 701kB/s] \n"]},{"name":"stdout","output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 112, 112]           9,408\n","       BatchNorm2d-2         [-1, 64, 112, 112]             128\n","              ReLU-3         [-1, 64, 112, 112]               0\n","         MaxPool2d-4           [-1, 64, 56, 56]               0\n","            Conv2d-5           [-1, 64, 56, 56]          36,864\n","       BatchNorm2d-6           [-1, 64, 56, 56]             128\n","              ReLU-7           [-1, 64, 56, 56]               0\n","            Conv2d-8           [-1, 64, 56, 56]          36,864\n","       BatchNorm2d-9           [-1, 64, 56, 56]             128\n","             ReLU-10           [-1, 64, 56, 56]               0\n","       BasicBlock-11           [-1, 64, 56, 56]               0\n","           Conv2d-12           [-1, 64, 56, 56]          36,864\n","      BatchNorm2d-13           [-1, 64, 56, 56]             128\n","             ReLU-14           [-1, 64, 56, 56]               0\n","           Conv2d-15           [-1, 64, 56, 56]          36,864\n","      BatchNorm2d-16           [-1, 64, 56, 56]             128\n","             ReLU-17           [-1, 64, 56, 56]               0\n","       BasicBlock-18           [-1, 64, 56, 56]               0\n","           Conv2d-19          [-1, 128, 28, 28]          73,728\n","      BatchNorm2d-20          [-1, 128, 28, 28]             256\n","             ReLU-21          [-1, 128, 28, 28]               0\n","           Conv2d-22          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-23          [-1, 128, 28, 28]             256\n","           Conv2d-24          [-1, 128, 28, 28]           8,192\n","      BatchNorm2d-25          [-1, 128, 28, 28]             256\n","             ReLU-26          [-1, 128, 28, 28]               0\n","       BasicBlock-27          [-1, 128, 28, 28]               0\n","           Conv2d-28          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-29          [-1, 128, 28, 28]             256\n","             ReLU-30          [-1, 128, 28, 28]               0\n","           Conv2d-31          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-32          [-1, 128, 28, 28]             256\n","             ReLU-33          [-1, 128, 28, 28]               0\n","       BasicBlock-34          [-1, 128, 28, 28]               0\n","           Conv2d-35          [-1, 256, 14, 14]         294,912\n","      BatchNorm2d-36          [-1, 256, 14, 14]             512\n","             ReLU-37          [-1, 256, 14, 14]               0\n","           Conv2d-38          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-39          [-1, 256, 14, 14]             512\n","           Conv2d-40          [-1, 256, 14, 14]          32,768\n","      BatchNorm2d-41          [-1, 256, 14, 14]             512\n","             ReLU-42          [-1, 256, 14, 14]               0\n","       BasicBlock-43          [-1, 256, 14, 14]               0\n","           Conv2d-44          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-45          [-1, 256, 14, 14]             512\n","             ReLU-46          [-1, 256, 14, 14]               0\n","           Conv2d-47          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-48          [-1, 256, 14, 14]             512\n","             ReLU-49          [-1, 256, 14, 14]               0\n","       BasicBlock-50          [-1, 256, 14, 14]               0\n","           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n","      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n","             ReLU-53            [-1, 512, 7, 7]               0\n","           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n","      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n","           Conv2d-56            [-1, 512, 7, 7]         131,072\n","      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n","             ReLU-58            [-1, 512, 7, 7]               0\n","       BasicBlock-59            [-1, 512, 7, 7]               0\n","           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n","      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n","             ReLU-62            [-1, 512, 7, 7]               0\n","           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n","      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n","             ReLU-65            [-1, 512, 7, 7]               0\n","       BasicBlock-66            [-1, 512, 7, 7]               0\n","AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n","           Linear-68                    [-1, 2]           1,026\n","================================================================\n","Total params: 11,177,538\n","Trainable params: 1,026\n","Non-trainable params: 11,176,512\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 62.79\n","Params size (MB): 42.64\n","Estimated Total Size (MB): 106.00\n","----------------------------------------------------------------\n","ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=512, out_features=2, bias=True)\n",")\n"]}],"source":["# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception*]\n","model_name = \"resnet\"\n","\n","# Number of classes in the dataset (ant & bee)\n","# In transfer learning, we can change the outptu classes\n","num_classes = 2\n","\n","'''\n","    True (feature extraction) : only update the reshaped layer params,\n","    False(finetuning)         : finetune the whole model, \n","'''\n","\n","feature_extract = True  \n","\n","model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n","model_ft = model_ft.to(device)\n","\n","from torchsummary import summary\n","\n","summary(model_ft, (3,input_size,input_size))\n","\n","print(model_ft)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"WStHdh2KUBRC"},"source":["# Prepare Datasets: hymenoptera_data"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"BJN_phYB12sC"},"source":["Unzip hymenoptera_data.zip to create training data\n","\n","[hymenoptera_data](https://www.kaggle.com/datasets/ajayrana/hymenoptera-data) is a binary (Ants and Bees) classification dataset consisting of a small number of images."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1683618036484,"user":{"displayName":"민웅한","userId":"00573162135069363404"},"user_tz":-540},"id":"H7WI7GwfUiLm","outputId":"c91310af-93ad-46f2-80c5-c20f2ab7d594"},"outputs":[],"source":["# !unzip -nq data/hymenoptera_data.zip -d data"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Qo08bMfU2lEU"},"source":["The images in the prepared dataset have different sizes. In order to be used as a learning model, the following process is required.\n","\n","- Assign the images in the folder to training/test data for learning\n","- Same pre-processing as ImageNet data for input of learning model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":454,"status":"ok","timestamp":1683619012829,"user":{"displayName":"민웅한","userId":"00573162135069363404"},"user_tz":-540},"id":"83UxwTIIV-0m","outputId":"b099f603-5474-4d40-d551-3736491d76cc"},"outputs":[],"source":["# Top level data directory. Here we assume the format of the directory conforms \n","#   to the ImageFolder structure\n","data_dir = \"./hymenoptera_data\"\n","\n","# Data augmentation and normalization for training\n","# Just normalization for validation\n","# Normalized with ImageNet mean and variance\n","transform = {\n","    'train': transforms.Compose([\n","        transforms.RandomResizedCrop(input_size),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Resize(input_size),\n","        transforms.CenterCrop(input_size),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","}\n","\n","print(\"Initializing Datasets and Dataloaders...\")\n","\n","training_data = torchvision.datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=transform['train'])\n","test_data = torchvision.datasets.ImageFolder(os.path.join(data_dir, 'val'), transform=transform['val'])\n","\n","classes = ['ant', 'bee']\n","print(f\"train dataset length = {len(training_data)}\")\n","print(f\"test  dataset length = {len(test_data)}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"wJR2fBj84oAA"},"source":["Use DataLoader to make dataset iterable."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4309,"status":"ok","timestamp":1683619023014,"user":{"displayName":"민웅한","userId":"00573162135069363404"},"user_tz":-540},"id":"DCHtaMbZUfDA","outputId":"3314a22e-651e-43b9-8f56-38acb8b6107a"},"outputs":[],"source":["# Batch size for training (change depending on how much memory you have)\n","# Meaning : 8 images per iteration\n","batch_size = 8\n","\n","train_dataloader = torch.utils.data.DataLoader(training_data, batch_size=batch_size, shuffle=True)\n","test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n","\n","for X, y in test_dataloader:\n","    print(f\"Shape of X [N, C, H, W]: {X.shape} {y.dtype}\")\n","    print(f\"Shape of y: {y.shape} {y.dtype}\")\n","    break"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"jYEEjueXfVJI"},"source":["# Optimization Setup"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"QCLJEDUs40bd"},"source":["### Optmizer function \n"," Gradient descent is the common optimisation strategy used in neural networks. Many of the variants and advanced optimisation functions now are available, \n","  \n","- Stochastic Gradient Descent, Adagrade, Adam, etc"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"9RL7WCYp483c"},"source":["### Loss function\n","\n","- Linear regression->Mean Squared Error\n","- Classification->Cross entropy"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1683619029997,"user":{"displayName":"민웅한","userId":"00573162135069363404"},"user_tz":-540},"id":"YFqXoS-YIL8I"},"outputs":[],"source":["loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model_ft.parameters(), lr = 0.001, momentum=0.9,weight_decay=5e-4)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"K4a5N0o7fXth"},"source":["# Train and Test the model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":134087,"status":"ok","timestamp":1683619185703,"user":{"displayName":"민웅한","userId":"00573162135069363404"},"user_tz":-540},"id":"XHjrOqIJWSVK","outputId":"847870bd-ead1-4e1e-ab9e-c2740ce6ad31"},"outputs":[],"source":["epochs = 10\n","for t in range(epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    train(train_dataloader, model_ft, loss_fn, optimizer, device, 15)\n","    test(test_dataloader, model_ft, loss_fn, device)\n","print(\"Done!\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"eegwkx0E5Psm"},"source":["# Visualize test results\n","\n","Select random test images and evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":539},"executionInfo":{"elapsed":2733,"status":"ok","timestamp":1683619213420,"user":{"displayName":"민웅한","userId":"00573162135069363404"},"user_tz":-540},"id":"opjyP_xXMAPe","outputId":"a00a222a-d7a2-43d1-b792-588b411f18f7"},"outputs":[],"source":["# Get some random test  images // BatchSize at a time\n","dataiter = iter(test_dataloader)\n","images, labels = next(dataiter)\n","\n","images = images.to(device)\n","labels = labels.to(device)\n","pred = model_ft(images)\n","predicted=pred.argmax(1);\n","\n","figure = plt.figure()\n","num_of_images = min(batch_size, 9)\n","\n","for index in range(num_of_images):\n","    plt.subplot(3, 3, index+1)\n","    plt.axis('off')    \n","    plt.title(f\"Ground Truth: {classes[labels[index]]}\")\n","    plt.title(f\"{classes[predicted[index].item()]} (true:{classes[labels[index]]})\")\n","    plt.imshow(np.transpose((images[index] * 0.224  + 0.456).cpu().numpy().squeeze(), (1, 2, 0)))  # 출력을 위한 차원변환 (channels*rows*cols) -> (rows*cols*channels)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"bDZa3absBt0Y"},"source":["Plot heatmap (confusion matrix)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":676},"executionInfo":{"elapsed":2575,"status":"ok","timestamp":1683619219430,"user":{"displayName":"민웅한","userId":"00573162135069363404"},"user_tz":-540},"id":"Z5dB7393NoVK","outputId":"bbcaa10c-b546-4f47-c23c-344035496034"},"outputs":[],"source":["heatmap = pd.DataFrame(data=0,index=classes,columns=classes)\n","with torch.no_grad():\n","    for images, labels in test_dataloader:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = model_ft(images)\n","        _, predicted = torch.max(outputs, 1)\n","        c = (predicted == labels).squeeze()\n","        for i in range(len(labels)):\n","            true_label = labels[i].item()\n","            predicted_label = predicted[i].item()\n","            heatmap.iloc[true_label,predicted_label] += 1\n","_, ax = plt.subplots(figsize=(10, 8))\n","ax = sns.heatmap(heatmap, annot=True, fmt=\"d\",cmap=\"YlGnBu\")\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"adO8bmjm5UcV"},"source":["# Saving Models\n","* save the structure of this class together with the model"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1683619223861,"user":{"displayName":"민웅한","userId":"00573162135069363404"},"user_tz":-540},"id":"Ren2lazx5WCo"},"outputs":[],"source":["torch.save(model_ft, f\"{model_name}_ft(hymenoptera).pth\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://github.com/ykkimhgu/DLIP-src/blob/main/Tutorial_Pytorch/Tutorial_PyTorch_T3_2_Transfer_Learning_using_Pre_trained_Models_(classification).ipynb","timestamp":1683614975462}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}
